{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T16:35:41.00311Z","iopub.status.busy":"2022-05-08T16:35:41.002788Z","iopub.status.idle":"2022-05-08T16:35:41.080678Z","shell.execute_reply":"2022-05-08T16:35:41.080052Z","shell.execute_reply.started":"2022-05-08T16:35:41.003074Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:18:33.394749Z","iopub.status.busy":"2022-05-08T15:18:33.394466Z","iopub.status.idle":"2022-05-08T15:18:36.486598Z","shell.execute_reply":"2022-05-08T15:18:36.485818Z","shell.execute_reply.started":"2022-05-08T15:18:33.394721Z"},"id":"bXilrZJoKw3-","scrolled":true,"trusted":true},"outputs":[],"source":["import nltk\n","import pickle\n","from PIL import Image\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Subset\n","\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","from torchvision import transforms\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data import random_split\n","from torchtext.data.metrics import bleu_score\n","\n","nltk.download('punkt')"]},{"cell_type":"markdown","metadata":{"_kg_hide-output":true,"id":"hsyV0N8RKw3x"},"source":["# Image Captioning "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:18:33.020151Z","iopub.status.busy":"2022-05-08T15:18:33.019871Z","iopub.status.idle":"2022-05-08T15:18:33.172977Z","shell.execute_reply":"2022-05-08T15:18:33.172151Z","shell.execute_reply.started":"2022-05-08T15:18:33.020115Z"},"id":"mdhVWYO9Kw39","outputId":"5623c4a7-fb09-494c-8183-40db6a871bc6","scrolled":true,"trusted":true},"outputs":[],"source":["def read_file(file_name, text_dir):\n","    with open(os.path.join(text_dir, file_name), 'rb') as files:\n","        lines = files.read().splitlines()\n","    return lines\n","\n","def map_imgs():\n","     img_cap_dict={}\n","     for caption in captions:\n","        caption = caption.decode(\"utf-8\")\n","        image_name = caption.split('#')[0]\n","        image_caption = caption.split('#')[1].split('\\t')[1]\n","        if image_name not in img_cap_dict.keys():\n","            img_cap_dict[image_name] = [image_caption]\n","     return img_cap_dict\n","\n","\n","text_dir = '/kaggle/input/flickr8k/Flickr8K/Flickr8k_text'\n","train_image_paths = read_file('Flickr_8k.trainImages.txt', text_dir)\n","test_image_paths = read_file('Flickr_8k.testImages.txt', text_dir)\n","captions = read_file('Flickr8k.token.txt', text_dir)\n","img_cap_dict=map_imgs()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:18:33.182051Z","iopub.status.busy":"2022-05-08T15:18:33.181354Z","iopub.status.idle":"2022-05-08T15:18:33.186265Z","shell.execute_reply":"2022-05-08T15:18:33.185648Z","shell.execute_reply.started":"2022-05-08T15:18:33.18201Z"},"id":"g0uJkR2aYNqp","scrolled":true,"trusted":true},"outputs":[],"source":["del img_cap_dict['2258277193_586949ec62.jpg.1'] # since there are odd number of images in the dataset (not a multiple of five)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:18:36.67037Z","iopub.status.busy":"2022-05-08T15:18:36.670102Z","iopub.status.idle":"2022-05-08T15:18:37.930364Z","shell.execute_reply":"2022-05-08T15:18:37.929658Z","shell.execute_reply.started":"2022-05-08T15:18:36.670333Z"},"id":"vTAhsJKTKw3_","outputId":"40ddb95d-617d-4b44-d71c-f283472a1d21","scrolled":true,"trusted":true},"outputs":[],"source":["class Vocab(object):\n","    def __init__(self):\n","        self.w2i={}\n","        self.i2w={}\n","        self.index=0\n","    \n","    def __call__(self,token):\n","        if not token in self.w2i:\n","            return self.w2i['<ukn>']\n","        return self.w2i[token]\n","    \n","    def __len__(self):\n","        return len(self.w2i)\n","    \n","    def add_token(self,token):\n","        if token not in self.w2i:\n","            self.w2i[token]=self.index\n","            self.i2w[self.index]=token\n","            self.index+=1\n","            \n","def build_vocabulary(map):\n","    counter=Counter()\n","    ids=map.keys()\n","    for i,id in enumerate(ids):\n","        captions=map[id]\n","        for caption in captions:\n","            tokens = nltk.tokenize.word_tokenize(caption.lower())\n","            counter.update(tokens)\n","    tokens = [token for token, cnt in counter.items()]\n","    vocab = Vocab()\n","    vocab.add_token('<pad>')\n","    vocab.add_token('<start>')\n","    vocab.add_token('<end>')\n","    vocab.add_token('<unk>')\n","    for i, token in enumerate(tokens):\n","        vocab.add_token(token)\n","    return vocab"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab = build_vocabulary(img_cap_dict)\n","vocab_path = '/kaggle/working/vocabulary.pkl'\n","with open(vocab_path, 'wb') as f:\n","    pickle.dump(vocab, f)"]},{"cell_type":"markdown","metadata":{"id":"GzhLFAe4Kw4A"},"source":["## Reshape Images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:18:37.948744Z","iopub.status.busy":"2022-05-08T15:18:37.948481Z","iopub.status.idle":"2022-05-08T15:20:56.552235Z","shell.execute_reply":"2022-05-08T15:20:56.551479Z","shell.execute_reply.started":"2022-05-08T15:18:37.948705Z"},"id":"Aykyvve0Kw4B","scrolled":true,"trusted":true},"outputs":[],"source":["def reshape_images(input_path, output_path, shape):\n","    if not os.path.exists(output_path):\n","        os.makedirs(output_path)\n","        \n","    images = os.listdir(input_path)\n","    num_im = len(images)\n","    \n","    for i, im in enumerate(images):\n","        with open(os.path.join(input_path, im), 'rb') as f:\n","            with Image.open(f) as image:\n","                image = image.resize(shape, Image.ANTIALIAS)\n","                image.save(os.path.join(output_path, im), image.format)\n","\n","input_path = '/kaggle/input/flickr8k/Flickr8K/Flicker8k_Images/'\n","output_path = '/kaggle/working/Flickr8K/resized_images/'\n","image_shape = [224, 224]\n","reshape_images(input_path, output_path, image_shape)"]},{"cell_type":"markdown","metadata":{"id":"kOFjmv6mKw4B"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:20:56.553713Z","iopub.status.busy":"2022-05-08T15:20:56.553469Z","iopub.status.idle":"2022-05-08T15:20:56.573574Z","shell.execute_reply":"2022-05-08T15:20:56.5729Z","shell.execute_reply.started":"2022-05-08T15:20:56.553678Z"},"id":"6OckNmVKKw4B","scrolled":true,"trusted":true},"outputs":[],"source":["class FlickrDataLoader(data.Dataset):\n","    def __init__(self,data_path,map, vocabulary,transform=None):\n","        self.root = data_path\n","        self.indices = list(map.keys())\n","        self.vocabulary = vocabulary\n","        self.transform = transform\n","        self.map=map\n","    \n","    def __getitem__(self, idx):\n","        vocabulary = self.vocabulary\n","        id = self.indices[idx]\n","        captions = self.map[id] \n","        image = Image.open(os.path.join(self.root,id)).convert('RGB')\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        caption = []\n","        caption.append(vocabulary('<start>'))\n","        for cap in captions:\n","            word_tokens = nltk.tokenize.word_tokenize(str(cap).lower())\n","            caption.extend([vocabulary(token) for token in word_tokens])       \n","        \n","        caption.append(vocabulary('<end>'))\n","        image_caption = torch.Tensor(caption)\n","        return image,image_caption\n"," \n","    def __len__(self):\n","        return len(self.indices)\n","def collate_function(data_batch):\n","    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n","    imgs, caps = zip(*data_batch)\n","    imgs = torch.stack(imgs, 0)\n","    cap_lens = [len(cap) for cap in caps]\n","    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n","    for i, cap in enumerate(caps):\n","        end = cap_lens[i]\n","        tgts[i, :end] = cap[:end]        \n","    return imgs, tgts, cap_lens\n"," \n","def get_loader(data_path, map, vocabulary, transform, batch_size, shuffle, num_workers):\n","    flicker_dataset = FlickrDataLoader(data_path=data_path,map=map,vocabulary=vocabulary,transform=transform)\n","    train_ds, test_ds = random_split(flicker_dataset, [7091,1000])\n","    train_data_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size,shuffle=shuffle,num_workers=num_workers,collate_fn=collate_function)\n","    test_data_loader = torch.utils.data.DataLoader(dataset=test_ds, batch_size=1,shuffle=shuffle,num_workers=num_workers,collate_fn=collate_function)\n","    return train_data_loader,test_data_loader"]},{"cell_type":"markdown","metadata":{"id":"UdHCNZptKw4C"},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:20:56.575856Z","iopub.status.busy":"2022-05-08T15:20:56.575174Z","iopub.status.idle":"2022-05-08T15:20:56.591457Z","shell.execute_reply":"2022-05-08T15:20:56.590752Z","shell.execute_reply.started":"2022-05-08T15:20:56.575818Z"},"id":"oDxDI0MiKw4C","scrolled":true,"trusted":true},"outputs":[],"source":["class CNNModel(nn.Module):\n","    def __init__(self, embedding_size):\n","        super(CNNModel, self).__init__()\n","        resnet = models.resnet152(pretrained=True)\n","        module_list = list(resnet.children())[:-1]\n","        self.resnet_module = nn.Sequential(*module_list)\n","        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n","        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n","        \n","    def forward(self, input_images):\n","        with torch.no_grad():\n","            resnet_features = self.resnet_module(input_images)\n","        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n","        final_features = self.batch_norm(self.linear_layer(resnet_features))\n","        return final_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n","        super(LSTMModel, self).__init__()\n","        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n","        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n","        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n","        self.max_seq_len = max_seq_len\n","        \n","    def forward(self, input_features, capts, lens):\n","        embeddings = self.embedding_layer(caps)\n","        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n","        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) \n","        hidden_variables, _ = self.lstm_layer(lstm_input)\n","        model_outputs = self.linear_layer(hidden_variables[0])\n","        return model_outputs\n","    \n","    def sample(self, input_features, lstm_states=None):\n","        sampled_indices = []\n","        lstm_inputs = input_features.unsqueeze(1)\n","        for i in range(self.max_seq_len):\n","            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)\n","            model_outputs = self.linear_layer(hidden_variables.squeeze(1))\n","            _, predicted_outputs = model_outputs.max(1)\n","            sampled_indices.append(predicted_outputs)\n","            lstm_inputs = self.embedding_layer(predicted_outputs)\n","            lstm_inputs = lstm_inputs.unsqueeze(1)\n","        sampled_indices = torch.stack(sampled_indices, 1)\n","        return sampled_indices"]},{"cell_type":"markdown","metadata":{"id":"mtxHOgGxKw4D"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:20:56.593925Z","iopub.status.busy":"2022-05-08T15:20:56.593735Z","iopub.status.idle":"2022-05-08T15:25:49.135774Z","shell.execute_reply":"2022-05-08T15:25:49.135006Z","shell.execute_reply.started":"2022-05-08T15:20:56.593895Z"},"id":"rGc8SJSRKw4E","outputId":"ac6ee0ae-c0b4-4d1a-8e58-1c944310b528","scrolled":true,"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if not os.path.exists('/kaggle/working/models/'):\n","    os.makedirs('/kaggle/working/models/')\n","\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","\n","with open('/kaggle/working/vocabulary.pkl', 'rb') as f:\n","    vocabulary = pickle.load(f)\n","\n","train_data_loader,test_data_loader = get_loader('/kaggle/working/Flickr8K/resized_images/', img_cap_dict, vocabulary, transform, 128,shuffle=True, num_workers=0) \n","\n","encoder_model = CNNModel(512).to(device)\n","decoder_model = LSTMModel(512, 512, len(vocabulary), 1).to(device) \n"," \n","loss_criterion = nn.CrossEntropyLoss()\n","parameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\n","optimizer = torch.optim.Adam(parameters, lr=0.001)\n","\n","\n","total_num_steps = len(train_data_loader)\n","for epoch in range(10):\n","    for i, (imgs, caps, lens) in enumerate(train_data_loader):\n","        imgs = imgs.to(device)\n","        caps = caps.to(device)\n","        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0] \n","        feats = encoder_model(imgs)\n","        outputs = decoder_model(feats, caps, lens)\n","        loss = loss_criterion(outputs, tgts)\n","        decoder_model.zero_grad()\n","        encoder_model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n"," \n","        if i % 10 == 0:\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                  .format(epoch, 5, i, total_num_steps, loss.item())) \n"," \n"]},{"cell_type":"markdown","metadata":{},"source":["### Save Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:25:49.142047Z","iopub.status.busy":"2022-05-08T15:25:49.140009Z","iopub.status.idle":"2022-05-08T15:25:49.476577Z","shell.execute_reply":"2022-05-08T15:25:49.475841Z","shell.execute_reply.started":"2022-05-08T15:25:49.142006Z"},"id":"S_a-56KUcIBL","scrolled":true,"trusted":true},"outputs":[],"source":["torch.save(decoder_model.state_dict(), os.path.join('/kaggle/working/models/', 'decoder-{}.ckpt'.format(epoch+1)))\n","torch.save(encoder_model.state_dict(), os.path.join('/kaggle/working/models/', 'encoder-{}.ckpt'.format(epoch+1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:25:49.484413Z","iopub.status.busy":"2022-05-08T15:25:49.482404Z","iopub.status.idle":"2022-05-08T15:25:49.49276Z","shell.execute_reply":"2022-05-08T15:25:49.492088Z","shell.execute_reply.started":"2022-05-08T15:25:49.484371Z"},"id":"inqLo56HKw4E","outputId":"926b21b7-2abd-45d5-f16f-fcfad179e1cf","scrolled":true,"trusted":true},"outputs":[],"source":["sum([7091, 1000])"]},{"cell_type":"markdown","metadata":{"id":"rnVAwwCVKw4E"},"source":["\n","## Prediction and BLEU Score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:25:49.499653Z","iopub.status.busy":"2022-05-08T15:25:49.497312Z","iopub.status.idle":"2022-05-08T15:26:06.403574Z","shell.execute_reply":"2022-05-08T15:26:06.402872Z","shell.execute_reply.started":"2022-05-08T15:25:49.499616Z"},"id":"oS81YQYHKw4E","scrolled":true,"trusted":true},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","with open('/kaggle/working/vocabulary.pkl', 'rb') as f:\n","    vocabulary = pickle.load(f)\n","\n","encoder_model = CNNModel(512).eval()\n","decoder_model = LSTMModel(512, 512, len(vocabulary), 1)\n","encoder_model = encoder_model.to(device)\n","decoder_model = decoder_model.to(device)\n","\n","encoder_model.load_state_dict(torch.load('/kaggle/working/models/encoder-10.ckpt'))\n","decoder_model.load_state_dict(torch.load('/kaggle/working/models/decoder-10.ckpt'))\n","\n","candidate_corpus=[]\n","reference_corpus=[]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, (imgs, caps, lens) in enumerate(test_data_loader):\n","    imgs = imgs.to(device)\n","    feat = encoder_model(imgs)\n","    sampled_indices = decoder_model.sample(feat)\n","    sampled_indices = sampled_indices.cpu().numpy()\n","    caps=caps.numpy()\n","    predicted_caption = []\n","    target_caption=[]\n","    for token_index in sampled_indices:\n","        for tokens in token_index:\n","            word = vocabulary.i2w[tokens]\n","            if word=='<unk>':\n","                continue\n","            if word=='<end>':\n","                continue\n","            if word == '<start>':\n","                continue\n","            if word== '.' :\n","                continue            \n","            predicted_caption.append(word)\n","    for token_index in caps:\n","        for tokens in token_index:\n","            word = vocabulary.i2w[tokens]\n","            if word=='<unk>':\n","                continue\n","            if word=='<end>':\n","                continue\n","            if word == '<start>':\n","                continue\n","            if word== '.' :\n","                continue \n","            target_caption.append(word)\n","            \n","    candidate_corpus.append(predicted_caption)\n","    reference_corpus.append([target_caption])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:06.405145Z","iopub.status.busy":"2022-05-08T15:26:06.404862Z","iopub.status.idle":"2022-05-08T15:26:07.174111Z","shell.execute_reply":"2022-05-08T15:26:07.173208Z","shell.execute_reply.started":"2022-05-08T15:26:06.405111Z"},"id":"4x6PAn0rKw4F","outputId":"ec8ca045-455d-4f8f-fed0-8070ec15fc21","scrolled":true,"trusted":true},"outputs":[],"source":["print(bleu_score(candidate_corpus, reference_corpus))"]},{"cell_type":"markdown","metadata":{"id":"bh3nr5DTKw4G"},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:07.264808Z","iopub.status.busy":"2022-05-08T15:26:07.264559Z","iopub.status.idle":"2022-05-08T15:26:07.277399Z","shell.execute_reply":"2022-05-08T15:26:07.276731Z","shell.execute_reply.started":"2022-05-08T15:26:07.264777Z"},"id":"H92FaEDxKw4G","outputId":"4b5ae94b-d3f7-4cb1-8e09-f76365762b23","trusted":true},"outputs":[],"source":["def predict_caption(image_file_path):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","    def load_image(image_file_path, transform=None):\n","        img = Image.open(image_file_path).convert('RGB')\n","        img = img.resize([224, 224], Image.LANCZOS)\n","\n","        if transform is not None:\n","            img = transform(img).unsqueeze(0)\n","\n","        return img\n","\n","    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","\n","    with open('/kaggle/working/vocabulary.pkl', 'rb') as f:\n","        vocabulary = pickle.load(f)\n","\n","    encoder_model = CNNModel(512).eval()\n","    decoder_model = LSTMModel(512, 512, len(vocabulary), 1)\n","    encoder_model = encoder_model.to(device)\n","    decoder_model = decoder_model.to(device)\n","\n","    encoder_model.load_state_dict(torch.load('/kaggle/working/models/encoder-10.ckpt'))\n","    decoder_model.load_state_dict(torch.load('/kaggle/working/models/decoder-10.ckpt'))\n","\n","    img = load_image(image_file_path, transform)\n","    img_tensor = img.to(device)\n","\n","    feat = encoder_model(img_tensor)\n","    sampled_indices = decoder_model.sample(feat)\n","    sampled_indices = sampled_indices[0].cpu().numpy()\n","    predicted_caption = []\n","    for token_index in sampled_indices:\n","        word = vocabulary.i2w[token_index]\n","        predicted_caption.append(word)\n","        if word == '<end>':\n","            break\n","    predicted_sentence = ' '.join(predicted_caption)\n","    \n","    return predicted_sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:07.989742Z","iopub.status.busy":"2022-05-08T15:26:07.989499Z","iopub.status.idle":"2022-05-08T15:26:07.99627Z","shell.execute_reply":"2022-05-08T15:26:07.995594Z","shell.execute_reply.started":"2022-05-08T15:26:07.989706Z"},"trusted":true},"outputs":[],"source":["def predict_captions(df):\n","    output_file = open(\"/kaggle/working/output_captions.txt\", \"w\")\n","    output_file.write(\"image,caption\\n\")\n","    for i in tqdm(df.index):\n","        img_file = df['image'][i]\n","        pred_caption = predict_caption('/kaggle/input/flickr8k/Flickr8K/Flicker8k_Images/' + img_file)\n","        output_file.write(img_file + \",\" + pred_caption + \"\\n\")\n","    output_file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:07.9979Z","iopub.status.busy":"2022-05-08T15:26:07.997508Z","iopub.status.idle":"2022-05-08T15:26:08.005889Z","shell.execute_reply":"2022-05-08T15:26:08.005088Z","shell.execute_reply.started":"2022-05-08T15:26:07.997861Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["data_set = '/kaggle/input/flickr8k/Flickr8K/'\n","captions_dir = data_set + \"Flickr8k_text/\"\n","train_images = captions_dir + 'Flickr_8k.trainImages.txt'\n","test_images = captions_dir + 'Flickr_8k.testImages.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:08.007439Z","iopub.status.busy":"2022-05-08T15:26:08.007247Z","iopub.status.idle":"2022-05-08T15:26:08.016984Z","shell.execute_reply":"2022-05-08T15:26:08.016228Z","shell.execute_reply.started":"2022-05-08T15:26:08.007416Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["def load_all_captions(file_name):\n","    text_file = open(file_name, \"r\")\n","    lines = text_file.readlines()\n","    data_set = []\n","    for l in range(len(lines)):\n","        line = lines[l].strip()\n","        image_name = line[:line.find(\"#\")]\n","        caption_number = line[line.find(\"#\")+1:line.find(\"#\")+2]\n","        caption = line[line.find(\"\\t\")+1:]\n","        data_set.append([image_name, caption_number, caption])\n","    return pd.DataFrame(data_set, columns =['image', 'caption#', 'caption'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:08.021553Z","iopub.status.busy":"2022-05-08T15:26:08.018Z","iopub.status.idle":"2022-05-08T15:26:08.119206Z","shell.execute_reply":"2022-05-08T15:26:08.118315Z","shell.execute_reply.started":"2022-05-08T15:26:08.021521Z"},"trusted":true},"outputs":[],"source":["captions_df = load_all_captions(captions_dir + \"Flickr8k.token.txt\")\n","captions_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:08.121049Z","iopub.status.busy":"2022-05-08T15:26:08.120772Z","iopub.status.idle":"2022-05-08T15:26:08.126499Z","shell.execute_reply":"2022-05-08T15:26:08.125615Z","shell.execute_reply.started":"2022-05-08T15:26:08.121012Z"},"trusted":true},"outputs":[],"source":["def load_test_image(file_name):\n","    text_file = open(file_name, \"r\")\n","    lines = text_file.readlines()\n","    data_set = []\n","    for l in range(len(lines)):\n","        line = lines[l].strip()\n","        data_set.append(line)\n","    return pd.DataFrame(data_set, columns =['image'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:08.128554Z","iopub.status.busy":"2022-05-08T15:26:08.128079Z","iopub.status.idle":"2022-05-08T15:26:08.143735Z","shell.execute_reply":"2022-05-08T15:26:08.142893Z","shell.execute_reply.started":"2022-05-08T15:26:08.128516Z"},"trusted":true},"outputs":[],"source":["test_df = load_test_image('/kaggle/input/flickr8k/Flickr8K/Flickr8k_text/Flickr_8k.testImages.txt')\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:08.145372Z","iopub.status.busy":"2022-05-08T15:26:08.144912Z","iopub.status.idle":"2022-05-08T15:26:08.151818Z","shell.execute_reply":"2022-05-08T15:26:08.15104Z","shell.execute_reply.started":"2022-05-08T15:26:08.145336Z"},"trusted":true},"outputs":[],"source":["def get_ground_captions(test_df, captions_df):\n","    new_df = pd.DataFrame(columns = ['image', 'caption'])\n","    for i in tqdm(test_df.index):\n","        temp = captions_df[captions_df['image']==test_df.iloc[i]['image']]\n","        for j in range(5):\n","            new_df = new_df.append({'image': test_df.iloc[i]['image'], 'caption': temp.iloc[j]['caption']}, ignore_index = True)\n","    return new_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:26:21.57278Z","iopub.status.busy":"2022-05-08T15:26:21.572521Z","iopub.status.idle":"2022-05-08T15:26:37.135964Z","shell.execute_reply":"2022-05-08T15:26:37.135294Z","shell.execute_reply.started":"2022-05-08T15:26:21.572751Z"},"trusted":true},"outputs":[],"source":["test_captions_df = get_ground_captions(test_df, captions_df)\n","test_captions_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-08T15:29:39.222667Z","iopub.status.busy":"2022-05-08T15:29:39.222404Z","iopub.status.idle":"2022-05-08T16:30:15.026972Z","shell.execute_reply":"2022-05-08T16:30:15.026171Z","shell.execute_reply.started":"2022-05-08T15:29:39.222639Z"},"trusted":true},"outputs":[],"source":["predict_captions(test_captions_df)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
